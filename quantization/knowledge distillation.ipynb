{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa11a941-7cbf-4552-8e6b-06ebe0390700",
   "metadata": {},
   "source": [
    "# Knowledge Distillation \n",
    "\n",
    "In this notebook we're going to take a pretrained ResNet18 and distill its knowledge to a smaller student network that only has a few layers using CIFAR10 dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65c4f97a-fa58-4b79-ab95-c76daafcd5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os \n",
    "\n",
    "import time \n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8771edfb-386e-4532-a6d7-139cfc0e5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Transforms for ResNet\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# 2. Load STL10 test set (we'll use test set for evaluation)\n",
    "train_ds = datasets.STL10(root=\"./data\", split=\"train\", download=True, transform=transform)\n",
    "test_ds  = datasets.STL10(root=\"./data\", split=\"test\",  download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,  num_workers=2)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "num_labels = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74155064-292a-4389-a04d-963baf8663e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # freeze all layers\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, num_labels)  # STL10 has 10 classes\n",
    "model.fc.requires_grad_(True)  # train only the classifier\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6112a10-38a6-4b93-adc3-7061d2a27009",
   "metadata": {},
   "source": [
    "Fine tune the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0441406-3a03-4bd9-9920-2cc0c0d57415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.2805\n",
      "Epoch 2: Loss = 0.4848\n",
      "Epoch 3: Loss = 0.3636\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):  # try 10–20 for best results\n",
    "    total_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a7ad229-79f8-4a0a-9697-011a4fdfb2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on STL10 test set: 93.05%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        preds = model(x).argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f\"Accuracy on STL10 test set: {correct / total * 100:.2f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), \"data/teacher_resnet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b7e299-6168-488a-8c7f-9317efd7c52d",
   "metadata": {},
   "source": [
    "## Student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fea2b41-6960-451e-8159-81199afdfe09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load teacher model\n",
    "teacher = models.resnet18(weights=None)\n",
    "teacher.fc = nn.Linear(teacher.fc.in_features, 10)\n",
    "teacher.load_state_dict(torch.load(\"data/teacher_resnet.pth\"))\n",
    "teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5658cd2e-2641-4352-b6de-79e29f8a0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class DeeperStudentCNN(nn.Module):\n",
    "    def __init__(self, num_labels=10):\n",
    "        super(DeeperStudentCNN, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 224 → 112\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 112 → 56\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 56 → 28\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 28 → 14\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 14 * 14, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "student = DeeperStudentCNN(num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e84df-96fe-48e1-9c5f-ec51213b50e4",
   "metadata": {},
   "source": [
    "Use distillation_loss to make student logits have similar probability distribution as the teacher_logits. This is a form of response based knowledge distillation. In addition we will use cross entropy to blend it in with the KL loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69419586-341b-447f-859c-29f58f609580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blended_distillation_loss(student_logits, teacher_logits, labels, T=2.0, alpha=0.7):\n",
    "    \"\"\"\n",
    "    student_logits: output from student model\n",
    "    teacher_logits: output from teacher model\n",
    "    labels        : true labels (int)\n",
    "    T             : temperature\n",
    "    alpha         : weight for soft loss (teacher)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Distillation (soft target) loss: KL divergence between softened distributions\n",
    "    soft_targets = F.softmax(teacher_logits / T, dim=1)\n",
    "    student_log_probs = F.log_softmax(student_logits / T, dim=1)\n",
    "    soft_loss = F.kl_div(student_log_probs, soft_targets, reduction='batchmean') * (T * T)\n",
    "\n",
    "    # 2. Hard loss: standard cross entropy with ground truth\n",
    "    hard_loss = F.cross_entropy(student_logits, labels)\n",
    "\n",
    "    # 3. Weighted combination\n",
    "    return alpha * soft_loss + (1.0 - alpha) * hard_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626a686-c7ac-48c0-a286-f8f05bca81c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "student.train()\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher(x)\n",
    "\n",
    "        student_logits = student(x)\n",
    "        loss = distillation_loss(student_logits, teacher_logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f2d000-4ac9-46ae-a357-e16700b0cb86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
